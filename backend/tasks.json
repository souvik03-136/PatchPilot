{"pull_requests": {"b468cd95-563d-49ca-9f48-1e5fa273b2bb": {"id": "b468cd95-563d-49ca-9f48-1e5fa273b2bb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:07.089049", "started_at": "2025-07-28T16:34:07.672717", "completed_at": "2025-07-28T16:35:08.611911", "results": {"security_issues": [{"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code explicitly instructs users to set API keys directly in the `.env` file.  This is a critical vulnerability as it exposes sensitive information if the repository is compromised or the `.env` file is accidentally committed to version control.", "line": 49, "file": "README.md", "confidence": 1.0}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The `.env.example` file likely contains placeholder API keys, which could be accidentally committed to version control, exposing sensitive information.", "line": 47, "file": "README.md", "confidence": 0.9}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code uses the `dotenv` library to load environment variables from a `.env` file.  While this is better than hardcoding secrets directly, it's still vulnerable if the `.env` file is committed to version control or otherwise accessible to unauthorized individuals.  Sensitive information like API keys for OpenAI or Ollama should never be stored in plain text.", "line": 28, "file": "src/main.py", "confidence": 1.0}, {"type": "Missing input validation", "severity": "medium", "description": "The code parses command-line arguments but lacks robust validation.  For example, the `--tickers` argument is split on commas, but there's no check to ensure the resulting tickers are valid or to prevent malicious input.  Similarly, date validation is minimal and could be bypassed with cleverly crafted input.", "line": 373, "file": "src/main.py", "confidence": 0.8}, {"type": "Missing input validation", "severity": "medium", "description": "The `parse_hedge_fund_response` function attempts to parse JSON, but its error handling is insufficient. While it catches `json.JSONDecodeError`, `TypeError`, and generic `Exception`, it doesn't prevent malicious or unexpected input from causing unexpected behavior or crashes.  More robust input validation and sanitization are needed before attempting JSON parsing.", "line": 52, "file": "src/main.py", "confidence": 0.9}], "quality_issues": [], "logic_issues": [{"file": "README.md", "analysis": "## Logic Analysis for README.md\n\n### Issues Found:\n\n1. **Issue Type**: Unclear Agent Interaction and Decision-Making Process\n   - Line:  Throughout the agent descriptions (lines 10-20) and the lack of detailed explanation of the `Portfolio Manager`'s decision-making process.\n   - Severity: Medium\n   - Fix: The README should provide a more detailed explanation of how the agents interact.  A flowchart or pseudocode illustrating the decision-making process, including how conflicting signals from different agents are resolved, would significantly improve clarity.  For example, how does the `Portfolio Manager` weigh the recommendations of the `Valuation Agent`, `Sentiment Agent`, etc.? What are the decision rules?  Does it use a weighted average, voting system, or another method?  The current description implies independent agents without a clear mechanism for integrating their outputs.\n\n2. **Issue Type**: Overreliance on LLMs without Validation\n   - Line: Throughout the description of the agents' functionalities.\n   - Severity: High\n   - Fix: The README acknowledges the use of LLMs but lacks discussion on validating the LLM outputs.  Financial decisions based solely on LLM outputs are extremely risky.  The system should incorporate mechanisms to verify the LLM-generated insights using traditional financial analysis techniques and data validation.  Adding a section on model validation and risk mitigation strategies is crucial.\n\n3. **Issue Type**:  Missing Error Handling and Robustness\n   - Line:  Throughout the usage instructions.\n   - Severity: Medium\n   - Fix: The README doesn't address potential errors during API calls (e.g., network issues, API rate limits, invalid API keys).  The code should include robust error handling to gracefully manage these situations and prevent crashes.  The README should mention how the system handles such scenarios.\n\n4. **Issue Type**:  Ambiguous Backtesting Methodology\n   - Line:  In the \"Running the Backtester\" section.\n   - Severity: Medium\n   - Fix: The README lacks details on the backtesting methodology.  What data is used? What performance metrics are calculated (e.g., Sharpe ratio, maximum drawdown)?  How are transaction costs handled?  A more precise description of the backtesting process is needed to assess its validity and reliability.\n\n5. **Issue Type**:  Potential for Bias in Agent Design\n   - Line:  Throughout the agent descriptions (lines 10-20).\n   - Severity: Medium\n   - Fix:  Naming agents after famous investors implicitly biases the system towards their investment philosophies.  The README should acknowledge this potential bias and discuss strategies to mitigate it, ensuring the system isn't overly reliant on specific investment styles.  Consider adding a section on fairness and bias mitigation.\n\n\n### Suggestions:\n\n- Add a section detailing the architecture of the system, including data flow diagrams and interaction between components.\n- Include a section on the limitations of the system and potential risks associated with using AI in financial markets.\n- Provide more details on the data sources used for training and testing the agents.\n- Add a section on the evaluation metrics used to assess the performance of the system.\n- Consider adding unit tests and integration tests to ensure the correctness and reliability of the code.\n-  Clearly state the assumptions made in the design and implementation of the system.\n-  Document the versioning strategy for the code and data.", "suggestions": [], "has_issues": true}, {"file": "src/backtester.py", "analysis": "## Logic Analysis for src/backtester.py\n\n### Issues Found:\n\n1. **Issue Type**: Incorrect Short Position Margin Calculation and Release\n   - Line: 171-184, 220-238\n   - Severity: High\n   - Fix: The margin calculation and release in `execute_trade` for short and cover actions are flawed.  The code calculates margin based on the *entire* short position, not just the newly shorted or covered quantity.  This leads to incorrect margin usage and available cash.  The margin should be calculated only for the shares being shorted or covered in each transaction.  The `portion` calculation in the `cover` section is also problematic and doesn't accurately reflect the margin used for the covered shares.  A more accurate approach would be to track margin usage per share shorted, and release margin on a per-share basis when covering.\n\n2. **Issue Type**: Potential for Division by Zero\n   - Line: 386, 458, 551\n   - Severity: Medium\n   - Fix: The code divides by `short_exposure` in calculating `long_short_ratio`, by `downside_std` in calculating `sortino_ratio`, and by `avg_loss` in calculating `win_loss_ratio`.  These variables can be zero, leading to a `ZeroDivisionError`. Add checks to handle these cases (e.g., using `if short_exposure > 1e-9` or similar).  Consider returning `np.inf` or `np.nan` as appropriate to represent these situations.\n\n3. **Issue Type**: Inconsistent Date Handling\n   - Line: 340, 343, 346\n   - Severity: Medium\n   - Fix: The code uses `previous_date_str` to fetch price data but doesn't handle cases where `previous_date_str` might be a weekend or holiday, leading to missing data.  It should use a more robust method to find the previous trading day.  The `pandas` library provides tools for this.\n\n4. **Issue Type**: Missing Error Handling in `get_price_data`\n   - Line: 355-362\n   - Severity: Medium\n   - Fix: The `try-except` block catches exceptions during price data fetching but only prints an error message and sets `missing_data` to `True`.  It should also handle the case where `price_data` is empty more gracefully, perhaps by logging the error and continuing to the next ticker or day.\n\n5. **Issue Type**:  Insufficient Data for Performance Metrics\n   - Line: 430-475\n   - Severity: Medium\n   - Fix: The `_update_performance_metrics` function checks if `len(clean_returns) < 2`, but this might not be sufficient.  Other metrics (like drawdown) also require a minimum amount of data.  Add more robust checks to ensure enough data points are available before calculating any performance metric.  Consider adding a minimum number of days for valid calculations.\n\n6. **Issue Type**:  Unhandled Exception in `get_price_data`\n   - Line: 355-362\n   - Severity: Medium\n   - Fix: The `try...except` block catches general exceptions during price data fetching.  It's better to catch specific exceptions (like `APIError` or `HTTPError`) for more informative error handling and logging.\n\n7. **Issue Type**: Potential for `IndexError` in `get_price_data`\n   - Line: 358\n   - Severity: Low\n   - Fix: Assumes `price_data` always has at least one row.  Add a check to ensure `len(price_data) > 0` before accessing `price_data.iloc[-1][\"close\"]`.\n\n\n### Suggestions:\n\n- **Improve Error Handling:** Implement more robust error handling throughout the code, especially for API calls and potential exceptions.  Log errors with timestamps and relevant context for debugging.\n- **Refactor `execute_trade`:**  Completely refactor the `execute_trade` function to simplify the logic and correctly handle margin for short positions.  Consider using a more object-oriented approach to represent positions and track margin usage.\n- **Use a dedicated date/time library:**  Instead of string manipulation, consistently use the `pandas` `Timestamp` objects for dates to avoid potential errors and inconsistencies.\n- **Add Unit Tests:** Write unit tests to thoroughly test the `Backtester` class and its methods, especially the `execute_trade` function and performance metric calculations.\n- **Enhance Logging:** Add more detailed logging to track the progress of the backtest, including trade executions, portfolio values, and any errors encountered.\n- **Input Validation:** Add more comprehensive input validation to the `__init__` method to ensure that the provided parameters are valid (e.g., check date formats, ensure `initial_capital` is positive, etc.).\n- **Memory Management:** While memory leaks are less likely in this code, consider using generators or iterators for large datasets to avoid loading everything into memory at once.  This is especially relevant for the data fetching functions.", "suggestions": [], "has_issues": true}], "decision": {"decision": "BLOCK", "risk_level": "critical", "summary": "3 critical security issues found", "recommendations": ["Fix critical issues immediately"], "total_issues": 5}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 31.2277991771698, "total_issues": 7}, "error": null}, "75953682-755b-4d5d-b42b-29b41bec7efb": {"id": "75953682-755b-4d5d-b42b-29b41bec7efb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:13.043160", "started_at": "2025-07-28T16:35:09.727602", "completed_at": "2025-07-28T16:35:51.209549", "results": {"security_issues": [], "quality_issues": [], "logic_issues": [], "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No critical issues found", "recommendations": ["No action required"], "total_issues": 0}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 21.346474647521973, "total_issues": 0}, "error": null}, "cba1180c-2b83-47d2-8794-d05f31515358": {"id": "cba1180c-2b83-47d2-8794-d05f31515358", "type": "security", "code_snippets": [{"file_path": "auth.py", "content": "import os\nimport sqlite3\nADMIN_PASSWORD = os.getenv(\"ADMIN_PASSWORD\", \"superSecret123!\")\ndef login(username, password):\n    if password == ADMIN_PASSWORD:\n        return True\n    return False\ndef get_user_data(user_id):\n    connection = sqlite3.connect(\"app.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n    result = cursor.fetchone()\n    connection.close()\n    return result", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:36:35.352153", "started_at": "2025-07-28T16:36:36.350113", "completed_at": "2025-07-28T16:36:39.519299", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing auth.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 17\n}\n]"], "analysis_method": "agent_system"}, "error": null}, "ae962b39-56dd-4590-879a-29a2c8d869bf": {"id": "ae962b39-56dd-4590-879a-29a2c8d869bf", "type": "security", "code_snippets": [{"file_path": "upload.py", "content": "def save_uploaded_file(file):\n    filename = file.filename\n    file.save('/uploads/' + filename)\n    return filename\ndef process_xml(data):\n    return eval(data)", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:37:25.769261", "started_at": "2025-07-28T16:37:26.654163", "completed_at": "2025-07-28T16:37:29.775578", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing upload.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 27\n}\n]"], "analysis_method": "agent_system"}, "error": null}}, "analysis_history": [{"id": "b468cd95-563d-49ca-9f48-1e5fa273b2bb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:07.089049", "started_at": "2025-07-28T16:34:07.672717", "completed_at": "2025-07-28T16:35:03.836194", "results": {"security_issues": [{"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code instructs users to create a `.env` file and store their OpenAI API key, Groq API key, and Financial Datasets API key directly within the file.  This is a critical vulnerability as storing API keys directly in the codebase or easily accessible files allows anyone with access to the file to gain unauthorized access to the services.", "line": 74, "file": "README.md", "confidence": 1.0}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code instructs users to set environment variables for API keys. While not directly hardcoded in the source code, this is still a security risk if the `.env` file is committed to version control or otherwise exposed.", "line": 78, "file": "README.md", "confidence": 1.0}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code uses the `dotenv` library to load environment variables from a `.env` file.  While this is better than hardcoding secrets directly,  it's still vulnerable if the `.env` file is committed to version control or otherwise accessible to unauthorized individuals.  API keys and other sensitive information should never be stored in version control or easily accessible locations.", "line": 28, "file": "src/main.py", "confidence": 1.0}, {"type": "Missing input validation", "severity": "medium", "description": "The code parses command-line arguments but lacks robust validation.  For example, the `--tickers` argument is split by commas, but there's no check to ensure the resulting tickers are valid or to prevent unexpected characters.  Similarly, date validation is minimal and could be bypassed with malformed input.", "line": 326, "file": "src/main.py", "confidence": 0.8}, {"type": "Missing input validation", "severity": "medium", "description": "The `parse_hedge_fund_response` function attempts to parse JSON, but its error handling is insufficient. While it catches `json.JSONDecodeError`, `TypeError`, and generic `Exception`, it doesn't prevent malicious or unexpected input from causing unexpected behavior or crashes.  More robust input validation and sanitization are needed to handle potentially harmful data.", "line": 51, "file": "src/main.py", "confidence": 0.9}], "quality_issues": [], "logic_issues": [{"file": "README.md", "analysis": "## Logic Analysis for README.md\n\n### Issues Found:\n\n1. **Issue Type**: Missing Error Handling and Robustness in Agent Interactions\n   - Line:  Throughout the `agents` directory (implied by project structure)\n   - Severity: High\n   - Fix:  The README highlights multiple agents making independent decisions.  The code (not shown) needs robust error handling.  What happens if one agent fails? Does the entire system halt?  Does the Portfolio Manager handle exceptions from other agents gracefully?  Consider implementing exception handling, retry mechanisms, and fallback strategies to ensure system stability and prevent cascading failures.  Logging should be comprehensive to aid debugging.\n\n2. **Issue Type**:  Over-reliance on External APIs and Lack of Redundancy\n   - Line: Setup section, API key requirements\n   - Severity: High\n   - Fix: The system relies heavily on external APIs (OpenAI, Groq, Financial Datasets).  A single API outage could cripple the entire system.  The design should incorporate redundancy and fallback mechanisms.  For example, using multiple LLM providers or caching financial data.  Consider using a circuit breaker pattern to prevent cascading failures from API issues.\n\n3. **Issue Type**:  Unclear Agent Interaction and Decision-Making Process\n   - Line: Description of agents (1-16)\n   - Severity: Medium\n   - Fix: The README describes various agents but lacks detail on how they interact and how the final trading decision is made.  A more detailed description of the workflow, including data flow between agents and conflict resolution mechanisms, is needed.  A flowchart or sequence diagram would improve clarity.\n\n4. **Issue Type**:  Lack of Backtesting Rigor and Validation\n   - Line: Usage - Running the Backtester\n   - Severity: High\n   - Fix: The backtester is mentioned, but the README doesn't specify the metrics used to evaluate performance (e.g., Sharpe ratio, maximum drawdown).  The backtesting period and data used should be clearly defined.  Overfitting is a significant risk in AI-driven trading; the README should address how overfitting is mitigated.  Out-of-sample testing is crucial and should be explicitly mentioned.\n\n5. **Issue Type**:  Potential for Bias and Unintended Consequences\n   - Line: Agent descriptions (1-16)\n   - Severity: Medium\n   - Fix:  The agents are named after famous investors, implying they will emulate their strategies.  However, the actual implementation might introduce biases or unintended consequences.  The README should acknowledge the potential for bias in the AI models and discuss strategies to mitigate it.  Regular audits and monitoring of agent behavior are necessary.\n\n6. **Issue Type**:  Missing Details on Data Preprocessing and Feature Engineering\n   - Line:  Throughout the README\n   - Severity: Medium\n   - Fix: The README lacks details on how financial data is preprocessed and what features are engineered for the agents.  Data quality and preprocessing significantly impact model performance.  This should be addressed.\n\n7. **Issue Type**:  Security Concerns with API Keys\n   - Line: Setup section, API key instructions\n   - Severity: Medium\n   - Fix:  The README instructs users to store API keys directly in the `.env` file.  This is a security risk.  Consider using more secure methods for managing API keys, such as environment variables set outside the repository or a dedicated secrets management system.\n\n\n### Suggestions:\n\n- **Detailed Architecture Diagram:** Create a comprehensive diagram illustrating the system architecture, data flow between agents, and decision-making process.\n- **Formal Specification of Agent Interactions:**  Define the interactions between agents formally (e.g., using a state machine or UML diagrams) to ensure clarity and prevent ambiguity.\n- **Comprehensive Testing Strategy:** Implement a robust testing strategy including unit tests, integration tests, and end-to-end tests to validate the system's functionality and identify potential bugs.\n- **Performance Monitoring and Logging:** Implement comprehensive logging and monitoring to track system performance, identify bottlenecks, and detect anomalies.\n- **Risk Assessment and Mitigation:** Conduct a thorough risk assessment to identify potential risks and develop mitigation strategies.  This should include financial, operational, and security risks.\n- **Ethical Considerations:**  Address ethical considerations related to AI in finance, including bias, fairness, and transparency.", "suggestions": [], "has_issues": true}, {"file": "src/backtester.py", "analysis": "## Logic Analysis for src/backtester.py\n\n### Issues Found:\n\n1. **Issue Type**: Incorrect Short Position Margin Calculation and Release\n   - Line: 171-184, 218-236\n   - Severity: High\n   - Fix: The margin calculation for short positions and the margin release upon covering are flawed.  The code calculates margin based on the *entire* short position's value, not just the newly shorted shares.  Similarly, margin release is calculated proportionally to the *entire* short position's margin, not just the covered shares. This leads to incorrect margin usage and cash balance.  The margin calculation and release should be adjusted to only consider the shares involved in the current trade.\n\n2. **Issue Type**: Potential for Division by Zero\n   - Line: 347, 448, 498\n   - Severity: Medium\n   - Fix: The code divides by `short_exposure` and `avg_loss` in several places.  If these values are zero, a `ZeroDivisionError` will occur. Add checks to handle these cases, perhaps returning `float('inf')` or a default value as appropriate.  The `_update_performance_metrics` function also has a check for `std_excess_return` and `downside_std`, but this should be extended to other potential division by zero errors.\n\n3. **Issue Type**: Inconsistent Handling of Missing Price Data\n   - Line: 304-320\n   - Severity: Medium\n   - Fix: The code handles missing price data by printing a warning and skipping the day. However, it doesn't consistently handle all potential exceptions during price data retrieval.  A more robust approach would be to use a `try-except` block around the entire price fetching loop, catching specific exceptions and handling them appropriately (e.g., logging errors, using fallback values, or raising a more informative exception).\n\n4. **Issue Type**:  Insufficient Error Handling in `get_price_data` calls\n   - Line: 309\n   - Severity: Medium\n   - Fix: The `get_price_data` function calls lack error handling.  Network issues or API problems could raise exceptions, causing the backtest to crash.  Wrap these calls in `try-except` blocks to catch and handle potential errors gracefully.\n\n5. **Issue Type**:  Unhandled Exceptions in `get_prices`, `get_financial_metrics`, `get_insider_trades`, `get_company_news`\n   - Line: 266-272\n   - Severity: Medium\n   - Fix: The `prefetch_data` function calls several API functions without error handling.  These calls should be wrapped in `try-except` blocks to handle potential exceptions (network errors, API rate limits, etc.) and prevent the prefetch from failing completely.  Consider logging errors and continuing the prefetch for other tickers.\n\n6. **Issue Type**:  Potential for Incorrect `max_consecutive_wins`/`max_consecutive_losses`\n   - Line: 517-522\n   - Severity: Low\n   - Fix: The calculation of `max_consecutive_wins` and `max_consecutive_losses` assumes that there is at least one day of data.  If `performance_df` is empty, this will raise an error.  Add a check to handle the empty case, perhaps setting these values to 0.\n\n7. **Issue Type**:  Hardcoded Risk-Free Rate\n   - Line: 430, 496\n   - Severity: Low\n   - Fix: The daily risk-free rate is hardcoded.  It's better to make this configurable or fetch it from a data source.\n\n8. **Issue Type**: Missing handling of empty `tickers` list.\n   - Line: 281, 290\n   - Severity: Medium\n   - Fix: The code assumes there will always be tickers.  Add a check to handle the case where the `tickers` list is empty to prevent errors.\n\n\n### Suggestions:\n\n- **Improve logging:** Implement more comprehensive logging throughout the code to track errors, warnings, and important events during the backtest.\n- **Unit testing:** Write unit tests to verify the correctness of individual functions, especially `execute_trade` and `calculate_portfolio_value`.\n- **Refactor `execute_trade`:** Break down the `execute_trade` function into smaller, more manageable functions to improve readability and maintainability.  This will also make testing easier.\n- **Add more comprehensive performance metrics:** Consider adding other performance metrics like Calmar ratio, Sterling ratio, and information ratio.\n- **Data validation:** Add input validation to ensure that the start and end dates are valid, the initial capital is positive, and the margin requirement is within a reasonable range (0 to 1).\n- **Use a more robust date handling library:** Consider using a more feature-rich date and time library like `pandas`'s `Timestamp` objects for more reliable date comparisons and calculations.  Avoid string manipulation of dates where possible.\n- **Parameterize the frequency:** Allow the user to specify the backtest frequency (daily, weekly, monthly).", "suggestions": [], "has_issues": true}], "decision": {"decision": "BLOCK", "risk_level": "critical", "summary": "3 critical security issues found", "recommendations": ["Fix critical issues immediately"], "total_issues": 5}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 33.00802040100098, "total_issues": 7}, "error": null}, {"id": "b468cd95-563d-49ca-9f48-1e5fa273b2bb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:07.089049", "started_at": "2025-07-28T16:34:07.672717", "completed_at": "2025-07-28T16:35:08.611911", "results": {"security_issues": [{"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code explicitly instructs users to set API keys directly in the `.env` file.  This is a critical vulnerability as it exposes sensitive information if the repository is compromised or the `.env` file is accidentally committed to version control.", "line": 49, "file": "README.md", "confidence": 1.0}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The `.env.example` file likely contains placeholder API keys, which could be accidentally committed to version control, exposing sensitive information.", "line": 47, "file": "README.md", "confidence": 0.9}, {"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code uses the `dotenv` library to load environment variables from a `.env` file.  While this is better than hardcoding secrets directly, it's still vulnerable if the `.env` file is committed to version control or otherwise accessible to unauthorized individuals.  Sensitive information like API keys for OpenAI or Ollama should never be stored in plain text.", "line": 28, "file": "src/main.py", "confidence": 1.0}, {"type": "Missing input validation", "severity": "medium", "description": "The code parses command-line arguments but lacks robust validation.  For example, the `--tickers` argument is split on commas, but there's no check to ensure the resulting tickers are valid or to prevent malicious input.  Similarly, date validation is minimal and could be bypassed with cleverly crafted input.", "line": 373, "file": "src/main.py", "confidence": 0.8}, {"type": "Missing input validation", "severity": "medium", "description": "The `parse_hedge_fund_response` function attempts to parse JSON, but its error handling is insufficient. While it catches `json.JSONDecodeError`, `TypeError`, and generic `Exception`, it doesn't prevent malicious or unexpected input from causing unexpected behavior or crashes.  More robust input validation and sanitization are needed before attempting JSON parsing.", "line": 52, "file": "src/main.py", "confidence": 0.9}], "quality_issues": [], "logic_issues": [{"file": "README.md", "analysis": "## Logic Analysis for README.md\n\n### Issues Found:\n\n1. **Issue Type**: Unclear Agent Interaction and Decision-Making Process\n   - Line:  Throughout the agent descriptions (lines 10-20) and the lack of detailed explanation of the `Portfolio Manager`'s decision-making process.\n   - Severity: Medium\n   - Fix: The README should provide a more detailed explanation of how the agents interact.  A flowchart or pseudocode illustrating the decision-making process, including how conflicting signals from different agents are resolved, would significantly improve clarity.  For example, how does the `Portfolio Manager` weigh the recommendations of the `Valuation Agent`, `Sentiment Agent`, etc.? What are the decision rules?  Does it use a weighted average, voting system, or another method?  The current description implies independent agents without a clear mechanism for integrating their outputs.\n\n2. **Issue Type**: Overreliance on LLMs without Validation\n   - Line: Throughout the description of the agents' functionalities.\n   - Severity: High\n   - Fix: The README acknowledges the use of LLMs but lacks discussion on validating the LLM outputs.  Financial decisions based solely on LLM outputs are extremely risky.  The system should incorporate mechanisms to verify the LLM-generated insights using traditional financial analysis techniques and data validation.  Adding a section on model validation and risk mitigation strategies is crucial.\n\n3. **Issue Type**:  Missing Error Handling and Robustness\n   - Line:  Throughout the usage instructions.\n   - Severity: Medium\n   - Fix: The README doesn't address potential errors during API calls (e.g., network issues, API rate limits, invalid API keys).  The code should include robust error handling to gracefully manage these situations and prevent crashes.  The README should mention how the system handles such scenarios.\n\n4. **Issue Type**:  Ambiguous Backtesting Methodology\n   - Line:  In the \"Running the Backtester\" section.\n   - Severity: Medium\n   - Fix: The README lacks details on the backtesting methodology.  What data is used? What performance metrics are calculated (e.g., Sharpe ratio, maximum drawdown)?  How are transaction costs handled?  A more precise description of the backtesting process is needed to assess its validity and reliability.\n\n5. **Issue Type**:  Potential for Bias in Agent Design\n   - Line:  Throughout the agent descriptions (lines 10-20).\n   - Severity: Medium\n   - Fix:  Naming agents after famous investors implicitly biases the system towards their investment philosophies.  The README should acknowledge this potential bias and discuss strategies to mitigate it, ensuring the system isn't overly reliant on specific investment styles.  Consider adding a section on fairness and bias mitigation.\n\n\n### Suggestions:\n\n- Add a section detailing the architecture of the system, including data flow diagrams and interaction between components.\n- Include a section on the limitations of the system and potential risks associated with using AI in financial markets.\n- Provide more details on the data sources used for training and testing the agents.\n- Add a section on the evaluation metrics used to assess the performance of the system.\n- Consider adding unit tests and integration tests to ensure the correctness and reliability of the code.\n-  Clearly state the assumptions made in the design and implementation of the system.\n-  Document the versioning strategy for the code and data.", "suggestions": [], "has_issues": true}, {"file": "src/backtester.py", "analysis": "## Logic Analysis for src/backtester.py\n\n### Issues Found:\n\n1. **Issue Type**: Incorrect Short Position Margin Calculation and Release\n   - Line: 171-184, 220-238\n   - Severity: High\n   - Fix: The margin calculation and release in `execute_trade` for short and cover actions are flawed.  The code calculates margin based on the *entire* short position, not just the newly shorted or covered quantity.  This leads to incorrect margin usage and available cash.  The margin should be calculated only for the shares being shorted or covered in each transaction.  The `portion` calculation in the `cover` section is also problematic and doesn't accurately reflect the margin used for the covered shares.  A more accurate approach would be to track margin usage per share shorted, and release margin on a per-share basis when covering.\n\n2. **Issue Type**: Potential for Division by Zero\n   - Line: 386, 458, 551\n   - Severity: Medium\n   - Fix: The code divides by `short_exposure` in calculating `long_short_ratio`, by `downside_std` in calculating `sortino_ratio`, and by `avg_loss` in calculating `win_loss_ratio`.  These variables can be zero, leading to a `ZeroDivisionError`. Add checks to handle these cases (e.g., using `if short_exposure > 1e-9` or similar).  Consider returning `np.inf` or `np.nan` as appropriate to represent these situations.\n\n3. **Issue Type**: Inconsistent Date Handling\n   - Line: 340, 343, 346\n   - Severity: Medium\n   - Fix: The code uses `previous_date_str` to fetch price data but doesn't handle cases where `previous_date_str` might be a weekend or holiday, leading to missing data.  It should use a more robust method to find the previous trading day.  The `pandas` library provides tools for this.\n\n4. **Issue Type**: Missing Error Handling in `get_price_data`\n   - Line: 355-362\n   - Severity: Medium\n   - Fix: The `try-except` block catches exceptions during price data fetching but only prints an error message and sets `missing_data` to `True`.  It should also handle the case where `price_data` is empty more gracefully, perhaps by logging the error and continuing to the next ticker or day.\n\n5. **Issue Type**:  Insufficient Data for Performance Metrics\n   - Line: 430-475\n   - Severity: Medium\n   - Fix: The `_update_performance_metrics` function checks if `len(clean_returns) < 2`, but this might not be sufficient.  Other metrics (like drawdown) also require a minimum amount of data.  Add more robust checks to ensure enough data points are available before calculating any performance metric.  Consider adding a minimum number of days for valid calculations.\n\n6. **Issue Type**:  Unhandled Exception in `get_price_data`\n   - Line: 355-362\n   - Severity: Medium\n   - Fix: The `try...except` block catches general exceptions during price data fetching.  It's better to catch specific exceptions (like `APIError` or `HTTPError`) for more informative error handling and logging.\n\n7. **Issue Type**: Potential for `IndexError` in `get_price_data`\n   - Line: 358\n   - Severity: Low\n   - Fix: Assumes `price_data` always has at least one row.  Add a check to ensure `len(price_data) > 0` before accessing `price_data.iloc[-1][\"close\"]`.\n\n\n### Suggestions:\n\n- **Improve Error Handling:** Implement more robust error handling throughout the code, especially for API calls and potential exceptions.  Log errors with timestamps and relevant context for debugging.\n- **Refactor `execute_trade`:**  Completely refactor the `execute_trade` function to simplify the logic and correctly handle margin for short positions.  Consider using a more object-oriented approach to represent positions and track margin usage.\n- **Use a dedicated date/time library:**  Instead of string manipulation, consistently use the `pandas` `Timestamp` objects for dates to avoid potential errors and inconsistencies.\n- **Add Unit Tests:** Write unit tests to thoroughly test the `Backtester` class and its methods, especially the `execute_trade` function and performance metric calculations.\n- **Enhance Logging:** Add more detailed logging to track the progress of the backtest, including trade executions, portfolio values, and any errors encountered.\n- **Input Validation:** Add more comprehensive input validation to the `__init__` method to ensure that the provided parameters are valid (e.g., check date formats, ensure `initial_capital` is positive, etc.).\n- **Memory Management:** While memory leaks are less likely in this code, consider using generators or iterators for large datasets to avoid loading everything into memory at once.  This is especially relevant for the data fetching functions.", "suggestions": [], "has_issues": true}], "decision": {"decision": "BLOCK", "risk_level": "critical", "summary": "3 critical security issues found", "recommendations": ["Fix critical issues immediately"], "total_issues": 5}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 31.2277991771698, "total_issues": 7}, "error": null}, {"id": "75953682-755b-4d5d-b42b-29b41bec7efb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:13.043160", "started_at": "2025-07-28T16:35:09.727602", "completed_at": "2025-07-28T16:35:16.204994", "results": {"security_issues": [{"type": "Hardcoded passwords/secrets/API keys", "severity": "critical", "description": "The code explicitly instructs users to set API keys directly in the `.env` file.  This is a critical vulnerability as these keys could be accidentally committed to version control or exposed through other means, granting unauthorized access to sensitive resources like OpenAI, Groq, and Financial Datasets APIs.", "line": 56, "file": "README.md", "confidence": 1.0}], "quality_issues": [], "logic_issues": [], "decision": {"decision": "BLOCK", "risk_level": "critical", "summary": "1 critical security issues found", "recommendations": ["Fix critical issues immediately"], "total_issues": 1}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 19.01432728767395, "total_issues": 1}, "error": null}, {"id": "75953682-755b-4d5d-b42b-29b41bec7efb", "type": "pr", "pr_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "status": "completed", "mode": "standard", "created_at": "2025-07-28T16:34:13.043160", "started_at": "2025-07-28T16:35:09.727602", "completed_at": "2025-07-28T16:35:51.209549", "results": {"security_issues": [], "quality_issues": [], "logic_issues": [], "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No critical issues found", "recommendations": ["No action required"], "total_issues": 0}, "pr_details": {"id": 2536944201, "number": 306, "title": "Argument --llm argument to main.py too", "body": "To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\r\n\r\n```bash\r\n# With Poetry:\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\npoetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\r\n\r\n# With Docker (on Linux/Mac):\r\n./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n\r\n# With Docker (on Windows):\r\nrun.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\r\n```\r\n\r\nIf you do not specify `--llm`, you will be prompted to select a provider and model interactively.\r\n\r\n\r\nREADME also updated", "state": "open", "created_at": "2025-05-22 11:43:33+00:00", "updated_at": "2025-05-22 11:44:42+00:00", "closed_at": null, "merged_at": null, "merge_commit_sha": null, "author": "Romamo", "assignees": [], "reviewers": [], "labels": [], "milestone": null, "base_branch": "main", "head_branch": "argument-llm-main", "base_sha": "ab8ce114e78e967948a3a48fe1206becc540e232", "head_sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "mergeable": false, "mergeable_state": "dirty", "merged": false, "comments": 0, "review_comments": 0, "commits": 5, "additions": 64, "deletions": 18, "changed_files": 3, "files": [{"filename": "README.md", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "patch": "@@ -147,6 +147,22 @@ run.bat --ticker AAPL,MSFT,NVDA main\n **Example Output:**\n <img width=\"992\" alt=\"Screenshot 2025-01-06 at 5 50 17 PM\" src=\"https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b\" />\n \n+To specify the LLM provider and model, use the `--llm` argument in the format `Provider:Model` (e.g., `Openai:gpt-4`, `Ollama:llama3`). This will override the interactive model selection.\n+\n+```bash\n+# With Poetry:\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+poetry run python src/main.py --ticker AAPL,MSFT,NVDA --llm Ollama:llama3\n+\n+# With Docker (on Linux/Mac):\n+./run.sh --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+\n+# With Docker (on Windows):\n+run.bat --ticker AAPL,MSFT,NVDA --llm Openai:gpt-4\n+```\n+\n+If you do not specify `--llm`, you will be prompted to select a provider and model interactively.\n+\n You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.\n \n ```bash\n@@ -205,6 +221,7 @@ run.bat --ticker AAPL,MSFT,NVDA backtest\n **Example Output:**\n <img width=\"941\" alt=\"Screenshot 2025-01-06 at 5 47 52 PM\" src=\"https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47\" />\n \n+To specify the LLM provider and model, use the `--llm` argument the same way as for the Hedge Fund.\n \n You can optionally specify the start and end dates to backtest over a specific time period.\n \n@@ -219,19 +236,6 @@ poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01\n run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest\n ```\n \n-You can also specify a `--ollama` flag to run the backtester using local LLMs.\n-```bash\n-# With Poetry:\n-poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama\n-\n-# With Docker (on Linux/Mac):\n-./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest\n-\n-# With Docker (on Windows):\n-run.bat --ticker AAPL,MSFT,NVDA --ollama backtest\n-```\n-\n-\n ## Project Structure \n ```\n ai-hedge-fund/", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/README.md", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/README.md?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/backtester.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "patch": "@@ -651,6 +651,12 @@ def analyze_performance(self):\n         action=\"store_true\",\n         help=\"Use all available analysts (overrides --analysts)\",\n     )\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -687,11 +693,26 @@ def analyze_performance(self):\n             selected_analysts = choices\n             print(f\"\\nSelected analysts: \" f\"{', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = None\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fbacktester.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fbacktester.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}, {"filename": "src/main.py", "status": "modified", "additions": 24, "deletions": 3, "changes": 27, "patch": "@@ -145,6 +145,12 @@ def create_workflow(selected_analysts=None):\n     parser.add_argument(\"--end-date\", type=str, help=\"End date (YYYY-MM-DD). Defaults to today\")\n     parser.add_argument(\"--show-reasoning\", action=\"store_true\", help=\"Show reasoning from each agent\")\n     parser.add_argument(\"--show-agent-graph\", action=\"store_true\", help=\"Show the agent graph\")\n+    parser.add_argument(\n+        \"--llm\",\n+        type=str,\n+        default=None,\n+        help=\"Specify LLM as provider:model (e.g., Openai:gpt-4, Ollama:llama3) to override interactive selection\",\n+    )\n     parser.add_argument(\"--ollama\", action=\"store_true\", help=\"Use Ollama for local LLM inference\")\n \n     args = parser.parse_args()\n@@ -176,11 +182,26 @@ def create_workflow(selected_analysts=None):\n         selected_analysts = choices\n         print(f\"\\nSelected analysts: {', '.join(Fore.GREEN + choice.title().replace('_', ' ') + Style.RESET_ALL for choice in choices)}\\n\")\n \n-    # Select LLM model based on whether Ollama is being used\n+    # Select LLM model based on argument or interactive selection\n     model_name = \"\"\n     model_provider = \"\"\n \n-    if args.ollama:\n+    # If command-line LLM is provided, use it directly\n+    if args.llm is not None:\n+        if ':' not in args.llm:\n+            print(f\"{Fore.RED}--llm argument must be in provider:model format (e.g., Openai:gpt-4){Style.RESET_ALL}\")\n+            sys.exit(1)\n+        model_provider, model_name = args.llm.split(':', 1)\n+        if model_provider.lower() == ModelProvider.OLLAMA.value:\n+            # For Ollama, ensure model is available\n+            if not ensure_ollama_and_model(model_name):\n+                print(f\"{Fore.RED}Cannot proceed without Ollama and the selected model.{Style.RESET_ALL}\")\n+                sys.exit(1)\n+            print(f\"\\nSelected {Fore.CYAN}Ollama{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+            model_provider = ModelProvider.OLLAMA.value\n+        else:\n+            print(f\"\\nSelected {Fore.CYAN}{model_provider}{Style.RESET_ALL} model: {Fore.GREEN + Style.BRIGHT}{model_name}{Style.RESET_ALL}\\n\")\n+    elif args.ollama:\n         print(f\"{Fore.CYAN}Using Ollama for local LLM inference.{Style.RESET_ALL}\")\n \n         # Select from Ollama-specific models\n@@ -232,7 +253,7 @@ def create_workflow(selected_analysts=None):\n         if not model_choice:\n             print(\"\\n\\nInterrupt received. Exiting...\")\n             sys.exit(0)\n-\n+        \n         model_name, model_provider = model_choice\n \n         # Get model info using the helper function", "blob_url": "https://github.com/virattt/ai-hedge-fund/blob/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "raw_url": "https://github.com/virattt/ai-hedge-fund/raw/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324/src%2Fmain.py", "contents_url": "https://api.github.com/repos/virattt/ai-hedge-fund/contents/src%2Fmain.py?ref=60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "commits_data": [{"sha": "f3a21a57084553621c55fc11ffd9dab683ac59dd", "message": "Added command line argument to choose llm model", "author": "Romamo", "date": "2025-05-22 08:44:25+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/f3a21a57084553621c55fc11ffd9dab683ac59dd"}, {"sha": "db3727cb36b95807089bfe943a8cf95da4c1294a", "message": "Removed wrongly added line", "author": "Romamo", "date": "2025-05-22 11:02:39+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/db3727cb36b95807089bfe943a8cf95da4c1294a"}, {"sha": "94e3627ac3890dd11d3f3894f019e12cf471c3b9", "message": "Updated readme", "author": "Romamo", "date": "2025-05-22 11:27:47+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/94e3627ac3890dd11d3f3894f019e12cf471c3b9"}, {"sha": "60e79c13a2ee4c49c9d0ab4257a932fba4870f9e", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:42:27+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60e79c13a2ee4c49c9d0ab4257a932fba4870f9e"}, {"sha": "60dc3133c3c89158a2c9fb7c80cd4f106dfb5324", "message": "Added --llm argument to main.py too", "author": "Romamo", "date": "2025-05-22 11:44:36+00:00", "url": "https://github.com/virattt/ai-hedge-fund/commit/60dc3133c3c89158a2c9fb7c80cd4f106dfb5324"}], "reviews": [], "html_url": "https://github.com/virattt/ai-hedge-fund/pull/306", "diff_url": "https://github.com/virattt/ai-hedge-fund/pull/306.diff", "patch_url": "https://github.com/virattt/ai-hedge-fund/pull/306.patch"}, "analysis_duration": 21.346474647521973, "total_issues": 0}, "error": null}, {"id": "cba1180c-2b83-47d2-8794-d05f31515358", "type": "security", "code_snippets": [{"file_path": "auth.py", "content": "import os\nimport sqlite3\nADMIN_PASSWORD = os.getenv(\"ADMIN_PASSWORD\", \"superSecret123!\")\ndef login(username, password):\n    if password == ADMIN_PASSWORD:\n        return True\n    return False\ndef get_user_data(user_id):\n    connection = sqlite3.connect(\"app.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n    result = cursor.fetchone()\n    connection.close()\n    return result", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:36:35.352153", "started_at": "2025-07-28T16:36:36.350113", "completed_at": "2025-07-28T16:36:38.490794", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing auth.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 18\n}\n]"], "analysis_method": "agent_system"}, "error": null}, {"id": "cba1180c-2b83-47d2-8794-d05f31515358", "type": "security", "code_snippets": [{"file_path": "auth.py", "content": "import os\nimport sqlite3\nADMIN_PASSWORD = os.getenv(\"ADMIN_PASSWORD\", \"superSecret123!\")\ndef login(username, password):\n    if password == ADMIN_PASSWORD:\n        return True\n    return False\ndef get_user_data(user_id):\n    connection = sqlite3.connect(\"app.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n    result = cursor.fetchone()\n    connection.close()\n    return result", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:36:35.352153", "started_at": "2025-07-28T16:36:36.350113", "completed_at": "2025-07-28T16:36:39.519299", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing auth.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 17\n}\n]"], "analysis_method": "agent_system"}, "error": null}, {"id": "ae962b39-56dd-4590-879a-29a2c8d869bf", "type": "security", "code_snippets": [{"file_path": "upload.py", "content": "def save_uploaded_file(file):\n    filename = file.filename\n    file.save('/uploads/' + filename)\n    return filename\ndef process_xml(data):\n    return eval(data)", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:37:25.769261", "started_at": "2025-07-28T16:37:26.654163", "completed_at": "2025-07-28T16:37:28.643391", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing upload.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 28\n}\n]"], "analysis_method": "agent_system"}, "error": null}, {"id": "ae962b39-56dd-4590-879a-29a2c8d869bf", "type": "security", "code_snippets": [{"file_path": "upload.py", "content": "def save_uploaded_file(file):\n    filename = file.filename\n    file.save('/uploads/' + filename)\n    return filename\ndef process_xml(data):\n    return eval(data)", "language": "python"}], "status": "completed", "created_at": "2025-07-28T16:37:25.769261", "started_at": "2025-07-28T16:37:26.654163", "completed_at": "2025-07-28T16:37:29.775578", "results": {"security_issues": [], "total_issues": 0, "severity_breakdown": {"critical": 0, "high": 0, "medium": 0, "low": 0}, "decision": {"decision": "APPROVE", "risk_level": "low", "summary": "No significant security issues found", "recommendations": ["No immediate action required"]}, "errors": ["Error analyzing upload.py: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 27\n}\n]"], "analysis_method": "agent_system"}, "error": null}]}